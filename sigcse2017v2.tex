% This is ''sig-alternate.tex'' V2.0 May 2012
% This file should be compiled with V2.5 of '\'sig-alternate.cls'' May 2012
%
% This example file demonstrates the use of the \'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The \'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.

\documentclass{sig-alternate}
\sloppy
\usepackage{paralist}
\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage[pdftex,breaklinks]{hyperref}
\usepackage{import}
\usepackage{tikz}
%\usepackage{caption}
%\captionsetup[figure]{skip=0pt}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SIGCSE}{2017 Seattle, Washington, USA}
\CopyrightYear{2017} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{An Analysis of Introductory University Programming Courses in the UK}
\iffalse
\numberofauthors{3}
\author{
% 1st. author
\alignauthor
Ellen Murphy\\
\affaddr{Institute for\\Mathematical Innovation}\\
\affaddr{University of Bath}\\
\affaddr{e.murphy@bath.ac.uk}
% 2nd. author
\alignauthor
Tom Crick\\
\affaddr{Dept. of Computing}\\
\affaddr{Cardiff Metropolitan University}\\
\affaddr{tcrick@cardiffmet.ac.uk}
% 3rd. author
\alignauthor
James H. Davenport\\
\affaddr{Dept. of Computer Science}\\
\affaddr{University of Bath}\\
\affaddr{j.h.davenport@bath.ac.uk}\\
}
\fi
\maketitle

\begin{abstract}
This paper reports the results of a survey of over 70 introductory
programming courses delivered at UK universities as part of their
first year computer science (or similar) degree programmes, conducted
in the first half of 2016. Results of this survey are compared with a
related survey conducted since 2010 (as well as earlier surveys from
2001 and 2003) at universities in Australia and New Zealand. Trends in
student numbers, programming paradigm, programming languages and
environment/tools used, as well as the reasons for choice of such are
reported. Other aspects of first programming courses such as
instructor experience, external delivery of courses and resources
given to students are also examined.

The results in this first UK survey indicate a trend towards...

...especially in the context of substantial computer science
curriculum reform in UK schools, as well as increasingly scrutiny of
teaching excellence and graduate employability for UK universities.
\end{abstract}

% check these...do we need keywords too?
% A category with the (minimum) three required fields
\category{K.3.2}{Computers \& Education}{Computer and Information Science Education}[Computer Science Education]
\category{K.4.1}{Computers And Society}{Public Policy Issues}
\keywords{Introductory Programming, Programming Languages, Programming
  Environments, Computer Science Education, Higher Education, Tertiary
  Education, UK}

\section{Introduction}\label{intro}

For many years -- and increasingly at all levels of compulsory and
post-compulsory education -- the choice of programming language to
introduce basic programming principles, constructs, syntax and
semantics has been regularly revisited. Even in the context of what
are perceived to be the most difficult introductory topics in computer
science degrees, numerous key themes across programming
appear~\cite{dale:2006}. 

So what is a good first programming language? The issues surrounding
choosing a first language~\cite{gupta:2004,kaplan:2010} -- and a
search of the ACM Digital Library identified a number of papers of the
form ``{\emph{X as a first programming language}}'', going as far back
as the 1980s -- as well as the potential impact on students' grades
and attainment~\cite{ivanovic-et-al:2015}.

Decades of research on the teaching of introductory programming has
had limited effect on classroom practice~\cite{pears-et-al:2007};
although relevant research exists across several disciplines including
education and cognitive science, disciplinary differences have made
this material inaccessible to many computing educators. Furthermore,
computer science instructors have not had access to comprehensive
surveys of research in this
area~\cite{mccracken-et-al:2001,pears-et-al:2007}.

However, in Australia and New Zealand there has been longitudinal data
collections~\cite{deraadt-et-al:2004,mason-et-al:2012,mason+cooper:2014}
surveying the teaching of introductory programming courses in higher
education. However, such surveys have not been conducted elsewhere on
this scale, and this paper reports the findings from running the first
such similar survey in the UK.

We remind the reader that the UK consists of four nations historically
ruled by one parliament, with an overall population of 64.5 million:
England (population: 54.3 million), Scotland (5.3 million), Wales (3.1
million) and Northern Ireland (1.8 million)~\cite{onspop:2016}. In
1997, Scotland and Wales held referendums which determined in both
cases the desire for self-government (along with Northern Ireland and
the 1998 Good Friday Agreement), creating assemblies to which a
variety of powers -- in particular, education -- were devolved from
the UK Parliament. In the context of international focus on curriculum
and qualification reform to support computer science education and
digital skills in schools, the four education systems of the UK have
proposed and implemented a variety of
changes~\cite{rs:2012,brown-et-al-toce2014}, particular in England,
with a new compulsory computing curriculum for ages 5-16 from
September 2014.

For universities across the UK offering computer science degrees, this
curriculum reform in schools has had uncertain (and emerging) impact
on their undergraduate programmes, with the diversity of educational
background of applicants likely to be increasing before it narrows: it
is certainly possible for prospective students to have anywhere from
zero to four or five years experience (and potentially two school
qualifications) in computer science with some knowledge of
programming.

% appropriate time frame?
Over the past three years, there has been increased scrutiny of the
quality of teaching in UK universities, partly linked to the current
levels -- and potential future increases -- of tuition fees (generally
paid by the student though government-supported loans), as well as the
perceived value of professional body accreditation and graduate
employability, especially for STEM disciplines. In February 2015, the
UK Department of Business, Innovation \& Skills and the Higher
Education Funding Council for England (HEFCE) initiated independent
reviews of STEM degree accreditation and graduate
employability\footnote{\url{https://www.gov.uk/government/collections/graduate-employment-and-accreditation-in-stem-independent-reviews}},
with a specific review -- the Shadbolt review~\cite{shadbolt:2016} --
focusing on computer science degree accreditation (in this case, with
BCS, The Chartered Institute for IT) and graduate employability,
reporting back in May 2016. Alongside a number of recommendations to
address the relatively high unemployment rates of computer sciences
graduates, particular on quality of data, course types, gender and
demographics, the Shadbolt review split generalist universities in
England into three bands, based on their average (across all subjects)
entrance tariff (qualifications of entrants); we have followed that
banding during our analysis the English results, so our data should
allow comparisons.

% should we mention TEF?
Alongside this increased scrutiny of standards and outputs for
computer science degrees in UK universities, a Teaching Excellence
Framework\footnote{\url{http://www.hefce.ac.uk/lt/tef/}} has been
proposed as part of proposed new higher education legislation. The
core ambition of the framework is ``to raise the quality and status of
teaching in higher education institutions''; excellence is to be
measured through a series of proxy metrics that include student
satisfaction, retention and graduate employability. There has been
significant concerns about the aims of the framework, as well as the
suitability of the metrics; more so in the context of it benchmarking
and creating leagues tables for ``teaching excellence'', as well as
deciding whether institutions are allowed to raise tuition fees in the
future. The UK's Higher Education Academy -- the national body which
champions teaching quality -- has previously supported initiatives for
improving learning \& teaching in computer science, including
innovative pedagogies for
programming~\cite{crick-et-al-hea:2015,davenport-et-al:latice2016},
but we have not seen the desired cascading of best practice and wider
national impact.
% should we mention HEA and our work?

In this emerging environment of policy, curricula, pedagogy and the
evolving demands of high-quality learning \& teaching for computer
science degree programmes, we present the findings from the first
national scale survey of introductory programming languages at UK
universities. Through this survey, we identify and analyse trends in
student numbers, programming paradigm, programming languages and
environment/tools used, as well as the reasons for choice of such are
reported. Other aspects of first programming courses such as
instructor experience, external delivery of courses and resources
given to students are also examined, along with comparisons to the
original Australasian surveys.

\section{Methodology}\label{method}

\subsection{Recruitment of Participants}

To recruit participants for the survey, a general invitation email was
sent to the Council of Professors and Heads of Computing (CPHC)
mailing list. CPHC have members from over 100 UK universities, and are
the representational body for this group in the
UK\footnote{\url{https://cphc.ac.uk/who-we-are/}}.The invitation
asked for the survey to be passed on to the most appropriate person to
complete it; Director of Studies, Chair of Teaching Committee or the
best fit for the individual institution. A couple of reminder circular
emails were also issued.

The survey was hosted online and was open from mid-May until the end
of June 2016, at which point it was closed and the results were
downloaded and analysed. Due to the recruitment method, there were
duplicate responses from some departments, and these were reconciled
by direct enquiry.

\subsection{Questions}

The questions used in the survey were generously provided by the
authors of~\cite{mason+cooper:2014}, so as to allow direct comparison
between the results of this survey and that of the Australian/New
Zealand 2014 survey. Where possible, questions were left unchanged,
although a small minority were edited to reflect the UK target
audience. 

As defined in~\cite{mason+cooper:2014}, text in the survey made clear
that the terminology ``course'' was used for ``the basic unit of study
that is completed by students towards a degree, usually studied over a
period pf a semester or session, in conjunction with other units of
study''.

The first section of the survey asked about the programming
language(s) in use, the reasons for their choice, and the perceived
difficulty and usefulness of the programming language(s). Following
this were questions regarding the use of environments or development
tools; which ones were used, the reasons for their choice and the
perceived difficulty. General questions about paradigm, instructor
experience and external delivery were asked, along with questions
regarding students receiving unauthorised assistance, and the
resources provided to students. Finally, participants were asked to
identify their top three main aims when teaching introductory
programming, and were also allowed to provide further comments.

% @James: I checked the paper - they could rank all reasons, not just top three.
In~\cite{mason+cooper:2014}, participants were asked to rank the
importance of the given reasons for choosing a programming language,
environment or tool. Due to technical limitations in online survey
tool used, it was not possible to do so in this survey, so our
Figure~\ref{fig:reasons} just reports counts.

Most questions were not mandatory; the exceptions were ``what
programming language(s) are in use?'' and a small number of feeder
questions to allow the survey to function correctly.

\section{Results and Discussion}\label{results}

\subsection{Universities and Courses}
%JHD: we need some discussion of response rates, either here or via
%Figure 1 also showing response rates, as in e-mail to Ellen

Upon completion of the survey, 155 instructors had, at least, started
the survey. Sixty-one of these dropped out before answering the
mandatory questions, and a further 14 were duplicates. Therefore, the
results presented here are drawn from the responses of 80 instructors
from at least 70 universities. Some participants did not answer all
questions and due to this the response rate varies by question.

Excluding the Open University's 3200 students, the participants in the
survey represented 13462 students, with a mean of 173 (but a standard
deviation of 88). Looking at Figure \ref{fig:TG} we see good response
rates, apart from the specialist HEIs (most of whom do not teach
computing) and the ``low tariff'' English ones. Fewer of these teach
computing, % e.g. all the 'arts' ones but we are not convinced that
this factor alone explains the response rate. In Northern Ireland, we
had responses from the two universities, but not the university
colleges, which are historically teacher-training colleges.

\begin{figure}
\begin{center}
\subimport{plots/}{tariffGroupCompare.tex}\vskip-12pt
\caption{The number of responding universities per Nation/   
 Tariff Group.\label{fig:TG}}
\end{center}
\end{figure}

%\subsection{Student Numbers}
%??
\begin{table}[]
\centering
\caption{The number of programming languages used in first programming courses.}
\label{tab:numLanguages}
\begin{tabular}{ccccc}
\hline
Languages & 1  & 2  & 3 & 4 \\ \hline
Courses   & 59 & 17 & 3 & 1 \\ \hline
\end{tabular}
\end{table}

\begin{figure}
\begin{center}
\subimport{plots/}{langPercentCompare.tex}
\end{center}\vskip-12pt
\caption{Language popularity by percentage of courses and students (excl. OU).\label{fig:lang}}
\end{figure}

\subsection{Languages}

This is the immediate major difference with
\cite{mason+cooper:2014}. Their survey showed a dead heat (27.3\% of
courses) between Java and Python, with Python winning (33.7\% to
26.9\%) when weighted by the number of students on the course.  Our
findings (Figure \ref{fig:lang}) show that Java is a clear winner by
any metric, being used in just under half the courses, while the
runner-up, Python, is in use in 13.2\%. The C family (C, C++ and C\#)
together scores 23.6\% by courses and 19.5\% by students. Figure
\ref{fig:lang} shows the effect of student-number weighting \emph{but}
we have excluded the Open University from this weighting, as its 3200
students learning Python (and Sense, a variant of Scratch) would have
distorted the comparison.

% ~\cite{guo:2014} -- US universities and Python being the most popular

\begin{figure}
\begin{center}
\subimport{plots/}{reasonsByCourseCompare.tex}
\end{center}
\caption{Reasons given for choosing a programming language by percentage for: all languages; Java; and Python.\label{fig:reasons}}
\end{figure}
%\par

Figure~\ref{fig:reasons} shows some of the reasons for this: Java
scores higher on ``relevance to industry'' and, perhaps somewhat
surprisingly, much higher on ``Object Oriented language''.
% @Tom: do you agree with ``somewhat surprisingly''?

Figure~\ref{fig;LangTariff} breaks down the choice of language by
nation and tariff group.  It is noticeable that the three English
tariff groups differ significantly, with Python outnumbering Java in
the low tariff universities, and C being almost exclusively in the
high tariff universities.

\begin{figure}
\begin{center}
\subimport{plots/}{UseAndDifficultyCompareLanguages.tex}
\end{center}\vskip-18pt
%\caption{The median of the perceived difficulty and usefulness of language, where 1 is `extremely easy' and 7 is `extremely difficult' for difficulty and 1 is `extremely useful' and 7 is `extremely useless' for usefulness.  Answers must have been given by at least two instructors.}
\caption{The median of the perceived difficulty and [pedagogic] usefulness of language, where 1 is `extremely easy/useful' and 7 is `extremely difficult/useless'.  Answers must have been given by at least two instructors.\label{fig:utility}}
\end{figure}

\begin{figure}
\begin{center}
\subimport{plots/}{langByTariffPercent.tex}
\end{center}
%\caption{The breakdown of programming languages for each of the Tariff Groups.}
%\end{figure}
%
%\begin{figure}
\begin{center}
\subimport{plots/}{TariffByLangPercent.tex}
\end{center}
\caption{The breakdown of programming languages by Nation and Tariff Groups.\label{fig;LangTariff}}
\end{figure}

\begin{table}[]
\centering
\caption{The main paradigm in use in the first programming course.}
\label{tab:paradigm}
\begin{tabular}{cccc}
\hline
Paradigm & Object-Oriented & Procedural & Functional \\ \hline
Courses  & 40              & 27         & 7          \\ \hline
\end{tabular}
\end{table}

%\clearpage

\begin{figure}
\begin{center}
\subimport{plots/}{paradigmByTariffPercent.tex}
\end{center}
\caption{The breakdown of the main paradigm in use for every Tariff Group.}
\end{figure}

%\begin{figure}
%\begin{center}
%\subimport{plots/}{TariffByParadigmPercent.tex}
%\end{center}
%\caption{The breakdown of Tariff Groups for each paradigm.}
%\end{figure}
%
%\begin{figure}
%\begin{center}
%\subimport{plots/}{langByParadigmPercent.tex}
%\end{center}\vskip-18pt
%\caption{The breakdown of programming languages in use for each paradigm.}
%\end{figure}


\begin{figure}
\begin{center}
\subimport{plots/}{ParadigmByLangPercent.tex}
\end{center}\vskip-18pt
\caption{The breakdown of the main paradigm in use for each programming language.}
\end{figure}


\begin{table}[h]
\centering
\caption{The number of years the instructor has been involved in teaching introductory programming.}
\label{tab:yearsTeaching}
\begin{tabular}{ccccccc}
\hline
Years       & \textless 2 & 2 - 5 & 5 - 10 & 10 - 20 & 20 - 30 & \textgreater 30 \\ \hline
Instructors & 3          & 9     & 9      & 27      & 19      & 7              \\ \hline
\end{tabular}
\end{table}

\subsection{Instructor Experience}

Participants were asked: ``How many years have you been involved in
teaching of introductory programming?''. The results, shown in
Table~\ref{tab:yearsTeaching}, indicate that of the survey
participants, the average was between 10 - 20 years.

\begin{table}[]
\centering
\caption{The number of tools/environments used in first programming courses.}
\label{tab:numTools}
\begin{tabular}{cccccc}
\hline
Tools   & 1  & 2  & 3 & 4 & 8 \\ \hline
Courses & 34 & 15 & 6 & 2 & 1 \\ \hline
\end{tabular}
\end{table}

\begin{figure}
\begin{center}
\subimport{plots/}{toolPercentCompare.tex}
\caption{Tool or environment popularity by percentage of courses and students.\label{fig:tools}}
\end{center}
\end{figure}

\subsection{IDEs and Tools}

Various data about IDEs and tools are collected in Figures
\ref{fig:tools}--\ref{fig:toolhard}. We note that, while Eclipse is
the most popular tool by some way (Figure \ref{fig:tools}), it is also
deemed to be most difficult (Figure \ref{fig:toolhard}). This,
apparently perverse, practice might be explained by the extent of
re-use of Eclipse in other courses (Figure \ref{fig:toolreuse}).

\begin{figure}
\begin{center}
\subimport{plots/}{reasonsByCourseCompareTool.tex}
\end{center}
\caption{Reasons given for choosing a tool or environment by percentage for: all tools and environments; BlueJ; and Eclipse.}
\end{figure}

\begin{figure}
\begin{center}
\subimport{plots/}{timingOtherCourseTool.tex}
\end{center}\vskip-18pt
\caption{For each tool or environment, whether it is used: for an initial part of the first programming course; throughout the whole of the first programming course; in any other course in the degree.\label{fig:toolreuse}}
\end{figure}


\begin{figure}
\begin{center}
\subimport{plots/}{DifficultyYouStudentsCompareTools.tex}
\end{center}
\caption{The median difficulty rating of tool/environment for the instructor and students to use, where 1 is `extremely easy' and 7 is `extremely difficult'.  Answers must have been given by at least two instructors.\label{fig:toolhard}}
\end{figure}

\subsection{Other Aspects of the Course}

\begin{figure}
\begin{center}
\subimport{plots/}{Steps.tex}
\end{center}\vskip-18pt
\caption{Steps taken to determine whether students have received unauthorised assistance on assignments.\label{fig:Plagiarise}}
\end{figure}
%\par

The questionnaire asked about the resources in terms of examples,
books etc. provided to students. The results are rather similar to
\cite[Figure 14]{mason+cooper:2014} so we do not repeat that here:
details are in the full paper.
%\par

Conversely, \cite{mason+cooper:2014} asked about {\bf @Ellen: exact
  question please} but didn't give the results  in their paper. We
report our results in Figure \ref{fig:Plagiarise}, as we think they
are of general interest. 

%\begin{figure}
%\begin{center}
%\subimport{plots/}{Resources.tex}
%\end{center}\vskip-18pt
%\caption{Resources provided to students.}
%\end{figure}

\subsection{Aims of an Introductory Programming Course}

 \cite{mason+cooper:2014} asked their respondents for the aims of
their introductory programming course. They, and we, asked for (up to)
three aims. The authors then attempted to classify the free-text
answers into the same categorisation as \cite{mason+cooper:2014}
used. While it is trivial to map the written aim ``Thinking
algorithmically'' to \cite{mason+cooper:2014}'s ``Algorithmic
thinking'' and so on, many were not so clear: for example, we mapped
``To learn a specific language'' to ``syntax/writing basic
code''. There were also a class of aims, such as ``Establish
professional software development practices'', that seemed very
coherent, but didn't map clearly to the \cite{mason+cooper:2014}
aims. These we have categorised as ``Software Engineering''.

\begin{figure}
\begin{center}
\subimport{plots/}{Aims.tex}
\end{center}\vskip-18pt
\caption{Aims of the introductory course. \label{fig:aims}}
\end{figure}

\section{General Discussion}\label{discussion}

\subsection{The U.K. context}

\subsection{Comparison with Australasia}

Here we compare with \cite{mason+cooper:2014}, the latest Australasian
survey. We have already commented on the major difference in language
choice, which colours many of the other comparisons. n fact, the
U.K.'s language choices seem more similar to Australasia's 2010
choices \cite{mason-et-al:2012} and \cite[Table 4]{mason+cooper:2014}
than even Australasia's 2013 choices. It is hard to know which comes
first, but we also notice that our difficulty/utility data (Figure
\ref{fig:utility}) is somewhat different from \cite[Figures
7,8]{mason+cooper:2014}

Another difference shows up in the tools/enviroments are: Figure
\ref{fig:tools} versus \cite{mason+cooper:2014}'s Figure 11. There,
``None'' and ``Other'' were the top two categories, with Idle, at
15\%, the most popular named product. In the UK, ``Other'' is fifth
and Idle seventh, with no ``None'' @Ellen: is that right?.


\section{Acknowledgements}

The authors would like to thank the participants for their engagement
with the survey, as well as the authors of [8] for providing us with
their survey and permission to use it.
% HEFCE? I think we should in the final version, but leave them out for now.
% CPHC? Not really
% GW4 - JHD has asked what the form of words should be.
We are grateful to the GW4 Alliance (Universities of Bath, Bristol,
Cardiff and Exeter) for funding the underpinning survey.
% bib
\bibliographystyle{abbrv}
\bibliography{sigcse2017}

\end{document}